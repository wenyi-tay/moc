{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOS-LT-Sentences.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPs8YMi7KentWvnjz9UYLLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenyi-tay/moc/blob/main/SOS_LT_Sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YlUK-oj0eAs2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import models, losses\n",
        "from sentence_transformers import LoggingHandler, SentenceTransformer, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, TripletEvaluator, BinaryClassificationEvaluator\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DVtLdlqgePXn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVj54vtzengu",
        "outputId": "d07f9638-ffe9-4719-b3ca-ba8d9c1c8e0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIuNNdEgep3K",
        "outputId": "49fa9b7c-fc9c-4720-bd13-25e8e5af748d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "qGVXgIUikBQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Triplet Loss"
      ],
      "metadata": {
        "id": "gexYGRtnmQko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.types import Device\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from typing import Union, Tuple, List, Iterable, Dict\n",
        "import torch.nn.functional as F\n",
        "from enum import Enum\n",
        "\n",
        "class TripletDistanceMetric(Enum):\n",
        "    \"\"\"\n",
        "    The metric for the triplet loss\n",
        "    \"\"\"\n",
        "    COSINE = lambda x, y: 1 - F.cosine_similarity(x, y)\n",
        "    EUCLIDEAN = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
        "    MANHATTAN = lambda x, y: F.pairwise_distance(x, y, p=1)"
      ],
      "metadata": {
        "id": "eq7DmJrdkBT1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedTripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements triplet loss. Given a triplet of (anchor, positive, negative),\n",
        "    the loss minimizes the distance between anchor and positive while it maximizes the distance\n",
        "    between anchor and negative. It compute the following loss function:\n",
        "    loss = max(||anchor - positive|| - ||anchor - negative|| + margin, 0).\n",
        "    Margin is an important hyperparameter and needs to be tuned respectively.\n",
        "    For further details, see: https://en.wikipedia.org/wiki/Triplet_loss\n",
        "    :param model: SentenceTransformerModel\n",
        "    :param distance_metric: Function to compute distance between two embeddings. The class TripletDistanceMetric contains common distance metrices that can be used.\n",
        "    :param triplet_margin: The negative should be at least this much further away from the anchor than the positive.\n",
        "    Example::\n",
        "        from sentence_transformers import SentenceTransformer,  SentencesDataset, LoggingHandler, losses\n",
        "        from sentence_transformers.readers import InputExample\n",
        "        model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "        train_examples = [InputExample(texts=['Anchor 1', 'Positive 1', 'Negative 1']),\n",
        "            InputExample(texts=['Anchor 2', 'Positive 2', 'Negative 2'])]\n",
        "        train_dataset = SentencesDataset(train_examples, model)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "        train_loss = losses.TripletLoss(model=model)\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 model: SentenceTransformer, \n",
        "                 distance_metric=TripletDistanceMetric.EUCLIDEAN, \n",
        "                 triplet_margin: float = 3, \n",
        "                 temperature = 0.30,\n",
        "                 alpha = 0.05):\n",
        "        super(SupervisedTripletLoss, self).__init__()\n",
        "        self.model = model\n",
        "        self.distance_metric = distance_metric\n",
        "        self.triplet_margin = triplet_margin\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def get_config_dict(self):\n",
        "        distance_metric_name = self.distance_metric.__name__\n",
        "        for name, value in vars(TripletDistanceMetric).items():\n",
        "            if value == self.distance_metric:\n",
        "                distance_metric_name = \"TripletDistanceMetric.{}\".format(name)\n",
        "                break\n",
        "\n",
        "        return {'distance_metric': distance_metric_name, 'triplet_margin': self.triplet_margin}\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]\n",
        "\n",
        "        rep_anchor, rep_pos, rep_neg = reps\n",
        "        distance_pos = self.distance_metric(rep_anchor, rep_pos)\n",
        "        distance_neg = self.distance_metric(rep_anchor, rep_neg)\n",
        "\n",
        "        ## The label determine whether the difference in the margin\n",
        "        ## For same category but different polarity, margin is small\n",
        "        ## For different category, regardless of the polarity, the margin is small\n",
        "        ## set a default margin of 5\n",
        "        ## margin is the additional margin for label = 1\n",
        "        ## when label = 1 means the negative example is of a different category\n",
        "        ## print(type(labels))\n",
        "\n",
        "\n",
        "        rep = torch.cat([rep_anchor, rep_pos, rep_neg], dim=0)\n",
        "\n",
        "        batch_size = rep.shape[0] ## \n",
        "\n",
        "        labels = torch.cat([labels, labels, labels], dim=0)\n",
        "        labels = labels.contiguous().view(-1, 1)\n",
        "\n",
        "        mask = torch.eq(labels, labels.T).float().to(device = \"cuda\")\n",
        "\n",
        "        contrast_feature = rep\n",
        "        anchor_feature = rep\n",
        "        anchor_count = 2 ## we have two views\n",
        "\n",
        "        # compute logits\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T),\n",
        "            self.temperature)\n",
        "        # print(\"Length of anchor dot product\")\n",
        "        # print(anchor_dot_contrast)\n",
        "\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size).view(-1, 1).to(device = \"cuda\"),\n",
        "            0\n",
        "        )\n",
        "        # print('Length of Logits Mask')\n",
        "        # print(logits_mask)\n",
        "\n",
        "        ## it produces 1 for the non-matching places and 0 for matching places i.e its opposite of mask\n",
        "        mask = mask * logits_mask\n",
        "        # compute log_prob with logsumexp\n",
        "\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "\n",
        "        ## log_prob = x - max(x1,..,xn) - logsumexp(x1,..,xn) the equation\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # compute mean of log-likelihood over positive\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
        "        # loss\n",
        "        supconloss = -1 * mean_log_prob_pos\n",
        "        supconloss = supconloss.mean()\n",
        "        #print('Supervised Contrastive Loss')\n",
        "        #print(supconloss)\n",
        "\n",
        "\n",
        "        triloss = F.relu(distance_pos - distance_neg + self.triplet_margin*0.1)\n",
        "        triloss = triloss.mean()\n",
        "        #print(\"Triplet Loss\")\n",
        "        #print(triloss)\n",
        "\n",
        "        combined_loss  = self.alpha*supconloss + (1-self.alpha)*triloss\n",
        "        #print(combined_loss)\n",
        "\n",
        "        return combined_loss"
      ],
      "metadata": {
        "id": "MEluZxrowXEk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dN1F_tmfkBXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triplet Evaluator Wenyi\n",
        "\n",
        "To include calculating loss in the evaluation dataset"
      ],
      "metadata": {
        "id": "kHwRH_Cs7RmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.evaluation import SentenceEvaluator, SimilarityFunction\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
        "from typing import List\n",
        "from sentence_transformers.readers import InputExample\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TripletEvaluatorWenyi(SentenceEvaluator):\n",
        "    \"\"\"\n",
        "    Evaluate a model based on a triplet: (sentence, positive_example, negative_example).\n",
        "        Checks if distance(sentence, positive_example) < distance(sentence, negative_example).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        anchors: List[str],\n",
        "        positives: List[str],\n",
        "        negatives: List[str],\n",
        "        main_distance_function: SimilarityFunction = None,\n",
        "        name: str = \"\",\n",
        "        batch_size: int = 16,\n",
        "        show_progress_bar: bool = False,\n",
        "        write_csv: bool = True,\n",
        "        triplet_margin = 5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :param anchors: Sentences to check similarity to. (e.g. a query)\n",
        "        :param positives: List of positive sentences\n",
        "        :param negatives: List of negative sentences\n",
        "        :param main_distance_function: One of 0 (Cosine), 1 (Euclidean) or 2 (Manhattan). Defaults to None, returning all 3.\n",
        "        :param name: Name for the output\n",
        "        :param batch_size: Batch size used to compute embeddings\n",
        "        :param show_progress_bar: If true, prints a progress bar\n",
        "        :param write_csv: Write results to a CSV file\n",
        "        :param triplet_margin: The margin for the triplet loss used for training\n",
        "        \"\"\"\n",
        "        self.anchors = anchors\n",
        "        self.positives = positives\n",
        "        self.negatives = negatives\n",
        "        self.name = name\n",
        "        self.triplet_margin = triplet_margin\n",
        "\n",
        "        assert len(self.anchors) == len(self.positives)\n",
        "        assert len(self.anchors) == len(self.negatives)\n",
        "\n",
        "        self.main_distance_function = main_distance_function\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        if show_progress_bar is None:\n",
        "            show_progress_bar = (\n",
        "                logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG\n",
        "            )\n",
        "        self.show_progress_bar = show_progress_bar\n",
        "\n",
        "        self.csv_file: str = \"triplet_evaluation\" + (\"_\" + name if name else \"\") + \"_results.csv\"\n",
        "        self.csv_headers = [\"epoch\", \"steps\", \"accuracy_cosinus\", \"accuracy_manhattan\", \"accuracy_euclidean\", \"triplet_loss_margin\", \"eval_loss\"]\n",
        "        self.write_csv = write_csv\n",
        "\n",
        "    @classmethod\n",
        "    def from_input_examples(cls, examples: List[InputExample], **kwargs):\n",
        "        anchors = []\n",
        "        positives = []\n",
        "        negatives = []\n",
        "\n",
        "        for example in examples:\n",
        "            anchors.append(example.texts[0])\n",
        "            positives.append(example.texts[1])\n",
        "            negatives.append(example.texts[2])\n",
        "        return cls(anchors, positives, negatives, **kwargs)\n",
        "\n",
        "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
        "        if epoch != -1:\n",
        "            if steps == -1:\n",
        "                out_txt = \" after epoch {}:\".format(epoch)\n",
        "            else:\n",
        "                out_txt = \" in epoch {} after {} steps:\".format(epoch, steps)\n",
        "        else:\n",
        "            out_txt = \":\"\n",
        "\n",
        "        logger.info(\"TripletEvaluator: Evaluating the model on \" + self.name + \" dataset\" + out_txt)\n",
        "\n",
        "        num_triplets = 0\n",
        "        num_correct_cos_triplets, num_correct_manhattan_triplets, num_correct_euclidean_triplets = 0, 0, 0\n",
        "\n",
        "        embeddings_anchors = model.encode(\n",
        "            self.anchors, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
        "        )\n",
        "        embeddings_positives = model.encode(\n",
        "            self.positives, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
        "        )\n",
        "        embeddings_negatives = model.encode(\n",
        "            self.negatives, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Cosine distance\n",
        "        pos_cos_distance = paired_cosine_distances(embeddings_anchors, embeddings_positives)\n",
        "        neg_cos_distances = paired_cosine_distances(embeddings_anchors, embeddings_negatives)\n",
        "\n",
        "        # Manhattan\n",
        "        pos_manhattan_distance = paired_manhattan_distances(embeddings_anchors, embeddings_positives)\n",
        "        neg_manhattan_distances = paired_manhattan_distances(embeddings_anchors, embeddings_negatives)\n",
        "\n",
        "        # Euclidean\n",
        "        pos_euclidean_distance = paired_euclidean_distances(embeddings_anchors, embeddings_positives)\n",
        "        neg_euclidean_distances = paired_euclidean_distances(embeddings_anchors, embeddings_negatives)\n",
        "\n",
        "        for idx in range(len(pos_cos_distance)):\n",
        "            num_triplets += 1\n",
        "\n",
        "            if pos_cos_distance[idx] < neg_cos_distances[idx]:\n",
        "                num_correct_cos_triplets += 1\n",
        "\n",
        "            if pos_manhattan_distance[idx] < neg_manhattan_distances[idx]:\n",
        "                num_correct_manhattan_triplets += 1\n",
        "\n",
        "            if pos_euclidean_distance[idx] < neg_euclidean_distances[idx]:\n",
        "                num_correct_euclidean_triplets += 1\n",
        "\n",
        "        accuracy_cos = num_correct_cos_triplets / num_triplets\n",
        "        accuracy_manhattan = num_correct_manhattan_triplets / num_triplets\n",
        "        accuracy_euclidean = num_correct_euclidean_triplets / num_triplets\n",
        "\n",
        "        logger.info(\"Accuracy Cosine Distance:   \\t{:.2f}\".format(accuracy_cos * 100))\n",
        "        logger.info(\"Accuracy Manhattan Distance:\\t{:.2f}\".format(accuracy_manhattan * 100))\n",
        "        logger.info(\"Accuracy Euclidean Distance:\\t{:.2f}\".format(accuracy_euclidean * 100))\n",
        "        \n",
        "        ## Wenyi added this <start>\n",
        "        triplet_margin = self.triplet_margin \n",
        "        pos_dist = torch.from_numpy(pos_cos_distance)\n",
        "        neg_dist = torch.from_numpy(neg_cos_distances)\n",
        "        losses = F.relu(pos_dist - neg_dist + (self.triplet_margin*0.1))\n",
        "        losses = losses.tolist()\n",
        "        # print(losses[0:50])\n",
        "        # to check that there should be zeros\n",
        "        eval_loss = sum(losses)\n",
        "        logger.info(\"Triplet Margin:             \\t{:.2f}\".format(self.triplet_margin))\n",
        "        logger.info(\"Evaluation Triplet Loss:    \\t{:.2f}\\n\".format(eval_loss))\n",
        "        ## Wenyi added this <end>\n",
        "\n",
        "\n",
        "        if output_path is not None and self.write_csv:\n",
        "            csv_path = os.path.join(output_path, self.csv_file)\n",
        "            if not os.path.isfile(csv_path):\n",
        "                with open(csv_path, newline=\"\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow(self.csv_headers)\n",
        "                    writer.writerow([epoch, steps, accuracy_cos, accuracy_manhattan, accuracy_euclidean, triplet_margin, eval_loss])\n",
        "\n",
        "            else:\n",
        "                with open(csv_path, newline=\"\", mode=\"a\", encoding=\"utf-8\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([epoch, steps, accuracy_cos, accuracy_manhattan, accuracy_euclidean, triplet_margin, eval_loss])\n",
        "\n",
        "        if self.main_distance_function == SimilarityFunction.COSINE:\n",
        "            return accuracy_cos\n",
        "        if self.main_distance_function == SimilarityFunction.MANHATTAN:\n",
        "            return accuracy_manhattan\n",
        "        if self.main_distance_function == SimilarityFunction.EUCLIDEAN:\n",
        "            return accuracy_euclidean\n",
        "\n",
        "        return max(accuracy_cos, accuracy_manhattan, accuracy_euclidean)"
      ],
      "metadata": {
        "id": "IvgbiPOH7RGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T-GreN0IkBcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q7ru-V6bkBfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Transformer Wenyi"
      ],
      "metadata": {
        "id": "NrI52t8Hm3jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import stat\n",
        "from collections import OrderedDict\n",
        "from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional\n",
        "import requests\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "import transformers\n",
        "from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download\n",
        "import torch\n",
        "from torch import nn, Tensor, device\n",
        "from torch.optim import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.multiprocessing as mp\n",
        "from tqdm.autonotebook import trange\n",
        "import math\n",
        "import queue\n",
        "import tempfile\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "from sentence_transformers import __MODEL_HUB_ORGANIZATION__\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "from sentence_transformers.util import import_from_string, batch_to_device, fullname, snapshot_download\n",
        "from sentence_transformers.models import Transformer, Pooling, Dense\n",
        "from sentence_transformers.model_card_templates import ModelCardTemplate\n",
        "from sentence_transformers import __version__\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SentenceTransformerWenyi(nn.Sequential):\n",
        "    \"\"\"\n",
        "    Loads or create a SentenceTransformer model, that can be used to map sentences / text to embeddings.\n",
        "\n",
        "    :param model_name_or_path: If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.\n",
        "    :param modules: This parameter can be used to create custom SentenceTransformer models from scratch.\n",
        "    :param device: Device (like 'cuda' / 'cpu') that should be used for computation. If None, checks if a GPU can be used.\n",
        "    :param cache_folder: Path to store models\n",
        "    :param use_auth_token: HuggingFace authentication token to download private models.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path: Optional[str] = None,\n",
        "                 modules: Optional[Iterable[nn.Module]] = None,\n",
        "                 device: Optional[str] = None,\n",
        "                 cache_folder: Optional[str] = None,\n",
        "                 use_auth_token: Union[bool, str, None] = None\n",
        "                 ):\n",
        "        self._model_card_vars = {}\n",
        "        self._model_card_text = None\n",
        "        self._model_config = {}\n",
        "\n",
        "        if cache_folder is None:\n",
        "            cache_folder = os.getenv('SENTENCE_TRANSFORMERS_HOME')\n",
        "            if cache_folder is None:\n",
        "                try:\n",
        "                    from torch.hub import _get_torch_home\n",
        "\n",
        "                    torch_cache_home = _get_torch_home()\n",
        "                except ImportError:\n",
        "                    torch_cache_home = os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n",
        "\n",
        "                cache_folder = os.path.join(torch_cache_home, 'sentence_transformers')\n",
        "\n",
        "        if model_name_or_path is not None and model_name_or_path != \"\":\n",
        "            logger.info(\"Load pretrained SentenceTransformer: {}\".format(model_name_or_path))\n",
        "\n",
        "            #Old models that don't belong to any organization\n",
        "            basic_transformer_models = ['albert-base-v1', 'albert-base-v2', 'albert-large-v1', 'albert-large-v2', 'albert-xlarge-v1', 'albert-xlarge-v2', 'albert-xxlarge-v1', 'albert-xxlarge-v2', 'bert-base-cased-finetuned-mrpc', 'bert-base-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'bert-base-multilingual-cased', 'bert-base-multilingual-uncased', 'bert-base-uncased', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking', 'bert-large-cased', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-uncased-whole-word-masking', 'bert-large-uncased', 'camembert-base', 'ctrl', 'distilbert-base-cased-distilled-squad', 'distilbert-base-cased', 'distilbert-base-german-cased', 'distilbert-base-multilingual-cased', 'distilbert-base-uncased-distilled-squad', 'distilbert-base-uncased-finetuned-sst-2-english', 'distilbert-base-uncased', 'distilgpt2', 'distilroberta-base', 'gpt2-large', 'gpt2-medium', 'gpt2-xl', 'gpt2', 'openai-gpt', 'roberta-base-openai-detector', 'roberta-base', 'roberta-large-mnli', 'roberta-large-openai-detector', 'roberta-large', 't5-11b', 't5-3b', 't5-base', 't5-large', 't5-small', 'transfo-xl-wt103', 'xlm-clm-ende-1024', 'xlm-clm-enfr-1024', 'xlm-mlm-100-1280', 'xlm-mlm-17-1280', 'xlm-mlm-en-2048', 'xlm-mlm-ende-1024', 'xlm-mlm-enfr-1024', 'xlm-mlm-enro-1024', 'xlm-mlm-tlm-xnli15-1024', 'xlm-mlm-xnli15-1024', 'xlm-roberta-base', 'xlm-roberta-large-finetuned-conll02-dutch', 'xlm-roberta-large-finetuned-conll02-spanish', 'xlm-roberta-large-finetuned-conll03-english', 'xlm-roberta-large-finetuned-conll03-german', 'xlm-roberta-large', 'xlnet-base-cased', 'xlnet-large-cased']\n",
        "\n",
        "            if os.path.exists(model_name_or_path):\n",
        "                #Load from path\n",
        "                model_path = model_name_or_path\n",
        "            else:\n",
        "                #Not a path, load from hub\n",
        "                if '\\\\' in model_name_or_path or model_name_or_path.count('/') > 1:\n",
        "                    raise ValueError(\"Path {} not found\".format(model_name_or_path))\n",
        "\n",
        "                if '/' not in model_name_or_path and model_name_or_path.lower() not in basic_transformer_models:\n",
        "                    # A model from sentence-transformers\n",
        "                    model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \"/\" + model_name_or_path\n",
        "\n",
        "                model_path = os.path.join(cache_folder, model_name_or_path.replace(\"/\", \"_\"))\n",
        "\n",
        "                # Download from hub with caching\n",
        "                snapshot_download(model_name_or_path,\n",
        "                                    cache_dir=cache_folder,\n",
        "                                    library_name='sentence-transformers',\n",
        "                                    library_version=__version__,\n",
        "                                    ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'],\n",
        "                                    use_auth_token=use_auth_token)\n",
        "\n",
        "            if os.path.exists(os.path.join(model_path, 'modules.json')):    #Load as SentenceTransformer model\n",
        "                modules = self._load_sbert_model(model_path)\n",
        "            else:   #Load with AutoModel\n",
        "                modules = self._load_auto_model(model_path)\n",
        "\n",
        "        if modules is not None and not isinstance(modules, OrderedDict):\n",
        "            modules = OrderedDict([(str(idx), module) for idx, module in enumerate(modules)])\n",
        "\n",
        "        super().__init__(modules)\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            logger.info(\"Use pytorch device: {}\".format(device))\n",
        "\n",
        "        self._target_device = torch.device(device)\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self, sentences: Union[str, List[str]],\n",
        "               batch_size: int = 32,\n",
        "               show_progress_bar: bool = None,\n",
        "               output_value: str = 'sentence_embedding',\n",
        "               convert_to_numpy: bool = True,\n",
        "               convert_to_tensor: bool = False,\n",
        "               device: str = None,\n",
        "               normalize_embeddings: bool = False) -> Union[List[Tensor], ndarray, Tensor]:\n",
        "        \"\"\"\n",
        "        Computes sentence embeddings\n",
        "\n",
        "        :param sentences: the sentences to embed\n",
        "        :param batch_size: the batch size used for the computation\n",
        "        :param show_progress_bar: Output a progress bar when encode sentences\n",
        "        :param output_value:  Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values\n",
        "        :param convert_to_numpy: If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.\n",
        "        :param convert_to_tensor: If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy\n",
        "        :param device: Which torch.device to use for the computation\n",
        "        :param normalize_embeddings: If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.\n",
        "\n",
        "        :return:\n",
        "           By default, a list of tensors is returned. If convert_to_tensor, a stacked tensor is returned. If convert_to_numpy, a numpy matrix is returned.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if show_progress_bar is None:\n",
        "            show_progress_bar = (logger.getEffectiveLevel()==logging.INFO or logger.getEffectiveLevel()==logging.DEBUG)\n",
        "\n",
        "        if convert_to_tensor:\n",
        "            convert_to_numpy = False\n",
        "\n",
        "        if output_value != 'sentence_embedding':\n",
        "            convert_to_tensor = False\n",
        "            convert_to_numpy = False\n",
        "\n",
        "        input_was_string = False\n",
        "        if isinstance(sentences, str) or not hasattr(sentences, '__len__'): #Cast an individual sentence to a list with length 1\n",
        "            sentences = [sentences]\n",
        "            input_was_string = True\n",
        "\n",
        "        if device is None:\n",
        "            device = self._target_device\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "        all_embeddings = []\n",
        "        length_sorted_idx = np.argsort([-self._text_length(sen) for sen in sentences])\n",
        "        sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
        "\n",
        "        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n",
        "            sentences_batch = sentences_sorted[start_index:start_index+batch_size]\n",
        "            features = self.tokenize(sentences_batch)\n",
        "            features = batch_to_device(features, device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out_features = self.forward(features)\n",
        "\n",
        "                if output_value == 'token_embeddings':\n",
        "                    embeddings = []\n",
        "                    for token_emb, attention in zip(out_features[output_value], out_features['attention_mask']):\n",
        "                        last_mask_id = len(attention)-1\n",
        "                        while last_mask_id > 0 and attention[last_mask_id].item() == 0:\n",
        "                            last_mask_id -= 1\n",
        "\n",
        "                        embeddings.append(token_emb[0:last_mask_id+1])\n",
        "                elif output_value is None:  #Return all outputs\n",
        "                    embeddings = []\n",
        "                    for sent_idx in range(len(out_features['sentence_embedding'])):\n",
        "                        row =  {name: out_features[name][sent_idx] for name in out_features}\n",
        "                        embeddings.append(row)\n",
        "                else:   #Sentence embeddings\n",
        "                    embeddings = out_features[output_value]\n",
        "                    embeddings = embeddings.detach()\n",
        "                    if normalize_embeddings:\n",
        "                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "                    # fixes for #522 and #487 to avoid oom problems on gpu with large datasets\n",
        "                    if convert_to_numpy:\n",
        "                        embeddings = embeddings.cpu()\n",
        "\n",
        "                all_embeddings.extend(embeddings)\n",
        "\n",
        "        all_embeddings = [all_embeddings[idx] for idx in np.argsort(length_sorted_idx)]\n",
        "\n",
        "        if convert_to_tensor:\n",
        "            all_embeddings = torch.stack(all_embeddings)\n",
        "        elif convert_to_numpy:\n",
        "            all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n",
        "\n",
        "        if input_was_string:\n",
        "            all_embeddings = all_embeddings[0]\n",
        "\n",
        "        return all_embeddings\n",
        "\n",
        "\n",
        "\n",
        "    def start_multi_process_pool(self, target_devices: List[str] = None):\n",
        "        \"\"\"\n",
        "        Starts multi process to process the encoding with several, independent processes.\n",
        "        This method is recommended if you want to encode on multiple GPUs. It is advised\n",
        "        to start only one process per GPU. This method works together with encode_multi_process\n",
        "\n",
        "        :param target_devices: PyTorch target devices, e.g. cuda:0, cuda:1... If None, all available CUDA devices will be used\n",
        "        :return: Returns a dict with the target processes, an input queue and and output queue.\n",
        "        \"\"\"\n",
        "        if target_devices is None:\n",
        "            if torch.cuda.is_available():\n",
        "                target_devices = ['cuda:{}'.format(i) for i in range(torch.cuda.device_count())]\n",
        "            else:\n",
        "                logger.info(\"CUDA is not available. Start 4 CPU worker\")\n",
        "                target_devices = ['cpu']*4\n",
        "\n",
        "        logger.info(\"Start multi-process pool on devices: {}\".format(', '.join(map(str, target_devices))))\n",
        "\n",
        "        ctx = mp.get_context('spawn')\n",
        "        input_queue = ctx.Queue()\n",
        "        output_queue = ctx.Queue()\n",
        "        processes = []\n",
        "\n",
        "        for cuda_id in target_devices:\n",
        "            p = ctx.Process(target=SentenceTransformer._encode_multi_process_worker, args=(cuda_id, self, input_queue, output_queue), daemon=True)\n",
        "            p.start()\n",
        "            processes.append(p)\n",
        "\n",
        "        return {'input': input_queue, 'output': output_queue, 'processes': processes}\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def stop_multi_process_pool(pool):\n",
        "        \"\"\"\n",
        "        Stops all processes started with start_multi_process_pool\n",
        "        \"\"\"\n",
        "        for p in pool['processes']:\n",
        "            p.terminate()\n",
        "\n",
        "        for p in pool['processes']:\n",
        "            p.join()\n",
        "            p.close()\n",
        "\n",
        "        pool['input'].close()\n",
        "        pool['output'].close()\n",
        "\n",
        "\n",
        "    def encode_multi_process(self, sentences: List[str], pool: Dict[str, object], batch_size: int = 32, chunk_size: int = None):\n",
        "        \"\"\"\n",
        "        This method allows to run encode() on multiple GPUs. The sentences are chunked into smaller packages\n",
        "        and sent to individual processes, which encode these on the different GPUs. This method is only suitable\n",
        "        for encoding large sets of sentences\n",
        "\n",
        "        :param sentences: List of sentences\n",
        "        :param pool: A pool of workers started with SentenceTransformer.start_multi_process_pool\n",
        "        :param batch_size: Encode sentences with batch size\n",
        "        :param chunk_size: Sentences are chunked and sent to the individual processes. If none, it determine a sensible size.\n",
        "        :return: Numpy matrix with all embeddings\n",
        "        \"\"\"\n",
        "\n",
        "        if chunk_size is None:\n",
        "            chunk_size = min(math.ceil(len(sentences) / len(pool[\"processes\"]) / 10), 5000)\n",
        "\n",
        "        logger.info(\"Chunk data into packages of size {}\".format(chunk_size))\n",
        "\n",
        "        input_queue = pool['input']\n",
        "        last_chunk_id = 0\n",
        "        chunk = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            chunk.append(sentence)\n",
        "            if len(chunk) >= chunk_size:\n",
        "                input_queue.put([last_chunk_id, batch_size, chunk])\n",
        "                last_chunk_id += 1\n",
        "                chunk = []\n",
        "\n",
        "        if len(chunk) > 0:\n",
        "            input_queue.put([last_chunk_id, batch_size, chunk])\n",
        "            last_chunk_id += 1\n",
        "\n",
        "        output_queue = pool['output']\n",
        "        results_list = sorted([output_queue.get() for _ in range(last_chunk_id)], key=lambda x: x[0])\n",
        "        embeddings = np.concatenate([result[1] for result in results_list])\n",
        "        return embeddings\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_multi_process_worker(target_device: str, model, input_queue, results_queue):\n",
        "        \"\"\"\n",
        "        Internal working process to encode sentences in multi-process setup\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            try:\n",
        "                id, batch_size, sentences = input_queue.get()\n",
        "                embeddings = model.encode(sentences, device=target_device,  show_progress_bar=False, convert_to_numpy=True, batch_size=batch_size)\n",
        "                results_queue.put([id, embeddings])\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "    def get_max_seq_length(self):\n",
        "        \"\"\"\n",
        "        Returns the maximal sequence length for input the model accepts. Longer inputs will be truncated\n",
        "        \"\"\"\n",
        "        if hasattr(self._first_module(), 'max_seq_length'):\n",
        "            return self._first_module().max_seq_length\n",
        "\n",
        "        return None\n",
        "\n",
        "    def tokenize(self, texts: Union[List[str], List[Dict], List[Tuple[str, str]]]):\n",
        "        \"\"\"\n",
        "        Tokenizes the texts\n",
        "        \"\"\"\n",
        "        return self._first_module().tokenize(texts)\n",
        "\n",
        "    def get_sentence_features(self, *features):\n",
        "        return self._first_module().get_sentence_features(*features)\n",
        "\n",
        "    def get_sentence_embedding_dimension(self):\n",
        "        for mod in reversed(self._modules.values()):\n",
        "            sent_embedding_dim_method = getattr(mod, \"get_sentence_embedding_dimension\", None)\n",
        "            if callable(sent_embedding_dim_method):\n",
        "                return sent_embedding_dim_method()\n",
        "        return None\n",
        "\n",
        "    def _first_module(self):\n",
        "        \"\"\"Returns the first module of this sequential embedder\"\"\"\n",
        "        return self._modules[next(iter(self._modules))]\n",
        "\n",
        "    def _last_module(self):\n",
        "        \"\"\"Returns the last module of this sequential embedder\"\"\"\n",
        "        return self._modules[next(reversed(self._modules))]\n",
        "\n",
        "    def save(self, path: str, model_name: Optional[str] = None, create_model_card: bool = True):\n",
        "        \"\"\"\n",
        "        Saves all elements for this seq. sentence embedder into different sub-folders\n",
        "        :param path: Path on disc\n",
        "        :param model_name: Optional model name\n",
        "        :param create_model_card: If True, create a README.md with basic information about this model\n",
        "        \"\"\"\n",
        "        if path is None:\n",
        "            return\n",
        "\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Save model to {}\".format(path))\n",
        "        modules_config = []\n",
        "\n",
        "        #Save some model info\n",
        "        if '__version__' not in self._model_config:\n",
        "            self._model_config['__version__'] = {\n",
        "                    'sentence_transformers': __version__,\n",
        "                    'transformers': transformers.__version__,\n",
        "                    'pytorch': torch.__version__,\n",
        "                }\n",
        "\n",
        "        with open(os.path.join(path, 'config_sentence_transformers.json'), 'w') as fOut:\n",
        "            json.dump(self._model_config, fOut, indent=2)\n",
        "\n",
        "        #Save modules\n",
        "        for idx, name in enumerate(self._modules):\n",
        "            module = self._modules[name]\n",
        "            if idx == 0 and isinstance(module, Transformer):    #Save transformer model in the main folder\n",
        "                model_path = path + \"/\"\n",
        "            else:\n",
        "                model_path = os.path.join(path, str(idx)+\"_\"+type(module).__name__)\n",
        "\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "            module.save(model_path)\n",
        "            modules_config.append({'idx': idx, 'name': name, 'path': os.path.basename(model_path), 'type': type(module).__module__})\n",
        "\n",
        "        with open(os.path.join(path, 'modules.json'), 'w') as fOut:\n",
        "            json.dump(modules_config, fOut, indent=2)\n",
        "\n",
        "        # Create model card\n",
        "        if create_model_card:\n",
        "            self._create_model_card(path, model_name)\n",
        "\n",
        "    def _create_model_card(self, path: str, model_name: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Create an automatic model and stores it in path\n",
        "        \"\"\"\n",
        "        if self._model_card_text is not None and len(self._model_card_text) > 0:\n",
        "            model_card = self._model_card_text\n",
        "        else:\n",
        "            tags = ModelCardTemplate.__TAGS__.copy()\n",
        "            model_card = ModelCardTemplate.__MODEL_CARD__\n",
        "\n",
        "            if len(self._modules) == 2 and isinstance(self._first_module(), Transformer) and isinstance(self._last_module(), Pooling) and self._last_module().get_pooling_mode_str() in ['cls', 'max', 'mean']:\n",
        "                pooling_module = self._last_module()\n",
        "                pooling_mode = pooling_module.get_pooling_mode_str()\n",
        "                model_card = model_card.replace(\"{USAGE_TRANSFORMERS_SECTION}\", ModelCardTemplate.__USAGE_TRANSFORMERS__)\n",
        "                pooling_fct_name, pooling_fct = ModelCardTemplate.model_card_get_pooling_function(pooling_mode)\n",
        "                model_card = model_card.replace(\"{POOLING_FUNCTION}\", pooling_fct).replace(\"{POOLING_FUNCTION_NAME}\", pooling_fct_name).replace(\"{POOLING_MODE}\", pooling_mode)\n",
        "                tags.append('transformers')\n",
        "\n",
        "            # Print full model\n",
        "            model_card = model_card.replace(\"{FULL_MODEL_STR}\", str(self))\n",
        "\n",
        "            # Add tags\n",
        "            model_card = model_card.replace(\"{TAGS}\", \"\\n\".join([\"- \"+t for t in tags]))\n",
        "\n",
        "            # Add dim info\n",
        "            self._model_card_vars[\"{NUM_DIMENSIONS}\"] = self.get_sentence_embedding_dimension()\n",
        "\n",
        "            # Replace vars we created while using the model\n",
        "            for name, value in self._model_card_vars.items():\n",
        "                model_card = model_card.replace(name, str(value))\n",
        "\n",
        "            # Replace remaining vars with default values\n",
        "            for name, value in ModelCardTemplate.__DEFAULT_VARS__.items():\n",
        "                model_card = model_card.replace(name, str(value))\n",
        "\n",
        "        if model_name is not None:\n",
        "            model_card = model_card.replace(\"{MODEL_NAME}\", model_name.strip())\n",
        "\n",
        "        with open(os.path.join(path, \"README.md\"), \"w\", encoding='utf8') as fOut:\n",
        "            fOut.write(model_card.strip())\n",
        "\n",
        "    def save_to_hub(self,\n",
        "                    repo_name: str,\n",
        "                    organization: Optional[str] = None,\n",
        "                    private: Optional[bool] = None,\n",
        "                    commit_message: str = \"Add new SentenceTransformer model.\",\n",
        "                    local_model_path: Optional[str] = None,\n",
        "                    exist_ok: bool = False,\n",
        "                    replace_model_card: bool = False):\n",
        "        \"\"\"\n",
        "        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.\n",
        "\n",
        "        :param repo_name: Repository name for your model in the Hub.\n",
        "        :param organization:  Organization in which you want to push your model or tokenizer (you must be a member of this organization).\n",
        "        :param private: Set to true, for hosting a prive model\n",
        "        :param commit_message: Message to commit while pushing.\n",
        "        :param local_model_path: Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded\n",
        "        :param exist_ok: If true, saving to an existing repository is OK. If false, saving only to a new repository is possible\n",
        "        :param replace_model_card: If true, replace an existing model card in the hub with the automatically created model card\n",
        "        :return: The url of the commit of your model in the given repository.\n",
        "        \"\"\"\n",
        "        token = HfFolder.get_token()\n",
        "        if token is None:\n",
        "            raise ValueError(\"You must login to the Hugging Face hub on this computer by typing `transformers-cli login`.\")\n",
        "\n",
        "        if '/' in repo_name:\n",
        "            splits = repo_name.split('/', maxsplit=1)\n",
        "            if organization is None or organization == splits[0]:\n",
        "                organization = splits[0]\n",
        "                repo_name = splits[1]\n",
        "            else:\n",
        "                raise ValueError(\"You passed and invalid repository name: {}.\".format(repo_name))\n",
        "\n",
        "        endpoint = \"https://huggingface.co\"\n",
        "        repo_url = HfApi(endpoint=endpoint).create_repo(\n",
        "                token,\n",
        "                repo_name,\n",
        "                organization=organization,\n",
        "                private=private,\n",
        "                repo_type=None,\n",
        "                exist_ok=exist_ok,\n",
        "            )\n",
        "        full_model_name = repo_url[len(endpoint)+1:].strip(\"/\")\n",
        "\n",
        "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "            # First create the repo (and clone its content if it's nonempty).\n",
        "            logger.info(\"Create repository and clone it if it exists\")\n",
        "            repo = Repository(tmp_dir, clone_from=repo_url)\n",
        "\n",
        "            # If user provides local files, copy them.\n",
        "            if local_model_path:\n",
        "                copy_tree(local_model_path, tmp_dir)\n",
        "            else:  # Else, save model directly into local repo.\n",
        "                create_model_card = replace_model_card or not os.path.exists(os.path.join(tmp_dir, 'README.md'))\n",
        "                self.save(tmp_dir, model_name=full_model_name, create_model_card=create_model_card)\n",
        "\n",
        "            #Find files larger 5M and track with git-lfs\n",
        "            large_files = []\n",
        "            for root, dirs, files in os.walk(tmp_dir):\n",
        "                for filename in files:\n",
        "                    file_path = os.path.join(root, filename)\n",
        "                    rel_path = os.path.relpath(file_path, tmp_dir)\n",
        "\n",
        "                    if os.path.getsize(file_path) > (5 * 1024 * 1024):\n",
        "                        large_files.append(rel_path)\n",
        "\n",
        "            if len(large_files) > 0:\n",
        "                logger.info(\"Track files with git lfs: {}\".format(\", \".join(large_files)))\n",
        "                repo.lfs_track(large_files)\n",
        "\n",
        "            logger.info(\"Push model to the hub. This might take a while\")\n",
        "            push_return = repo.push_to_hub(commit_message=commit_message)\n",
        "\n",
        "            def on_rm_error(func, path, exc_info):\n",
        "                # path contains the path of the file that couldn't be removed\n",
        "                # let's just assume that it's read-only and unlink it.\n",
        "                try:\n",
        "                    os.chmod(path, stat.S_IWRITE)\n",
        "                    os.unlink(path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Remove .git folder. On Windows, the .git folder might be read-only and cannot be deleted\n",
        "            # Hence, try to set write permissions on error\n",
        "            try:\n",
        "                for f in os.listdir(tmp_dir):\n",
        "                    shutil.rmtree(os.path.join(tmp_dir, f), onerror=on_rm_error)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Error when deleting temp folder: {}\".format(str(e)))\n",
        "                pass\n",
        "\n",
        "\n",
        "        return push_return\n",
        "\n",
        "    def smart_batching_collate(self, batch):\n",
        "        \"\"\"\n",
        "        Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n",
        "        Here, batch is a list of tuples: [(tokens, label), ...]\n",
        "\n",
        "        :param batch:\n",
        "            a batch from a SmartBatchingDataset\n",
        "        :return:\n",
        "            a batch of tensors for the model\n",
        "        \"\"\"\n",
        "        num_texts = len(batch[0].texts)\n",
        "        texts = [[] for _ in range(num_texts)]\n",
        "        labels = []\n",
        "        \n",
        "        for example in batch:\n",
        "\n",
        "            for idx, text in enumerate(example.texts):\n",
        "                texts[idx].append(text)\n",
        "\n",
        "            labels.append(example.label)\n",
        "        \n",
        "        labels = torch.tensor(labels).to(self._target_device)\n",
        "\n",
        "        sentence_features = []\n",
        "        for idx in range(num_texts):\n",
        "            tokenized = self.tokenize(texts[idx])\n",
        "            batch_to_device(tokenized, self._target_device)\n",
        "            sentence_features.append(tokenized)\n",
        "\n",
        "        return sentence_features, labels\n",
        "\n",
        "\n",
        "    def _text_length(self, text: Union[List[int], List[List[int]]]):\n",
        "        \"\"\"\n",
        "        Help function to get the length for the input text. Text can be either\n",
        "        a list of ints (which means a single text as input), or a tuple of list of ints\n",
        "        (representing several text inputs to the model).\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(text, dict):              #{key: value} case\n",
        "            return len(next(iter(text.values())))\n",
        "        elif not hasattr(text, '__len__'):      #Object has no len() method\n",
        "            return 1\n",
        "        elif len(text) == 0 or isinstance(text[0], int):    #Empty string or list of ints\n",
        "            return len(text)\n",
        "        else:\n",
        "            return sum([len(t) for t in text])      #Sum of length of individual strings\n",
        "\n",
        "    def fit(self,\n",
        "            train_objectives: Iterable[Tuple[DataLoader, nn.Module]],\n",
        "            evaluator: SentenceEvaluator = None,\n",
        "            epochs: int = 1,\n",
        "            steps_per_epoch = None,\n",
        "            scheduler: str = 'WarmupLinear',\n",
        "            warmup_steps: int = 10000,\n",
        "            optimizer_class: Type[Optimizer] = transformers.AdamW,\n",
        "            optimizer_params : Dict[str, object]= {'lr': 2e-5},\n",
        "            weight_decay: float = 0.01,\n",
        "            evaluation_steps: int = 0,\n",
        "            output_path: str = None,\n",
        "            save_best_model: bool = True,\n",
        "            max_grad_norm: float = 1,\n",
        "            use_amp: bool = False,\n",
        "            callback: Callable[[float, int, int], None] = None,\n",
        "            show_progress_bar: bool = True,\n",
        "            checkpoint_path: str = None,\n",
        "            checkpoint_save_steps: int = 500,\n",
        "            checkpoint_save_total_limit: int = 0\n",
        "            ):\n",
        "        \"\"\"\n",
        "        Train the model with the given training objective\n",
        "        Each training objective is sampled in turn for one batch.\n",
        "        We sample only as many batches from each objective as there are in the smallest one\n",
        "        to make sure of equal training with each dataset.\n",
        "\n",
        "        :param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n",
        "        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n",
        "        :param epochs: Number of epochs for training\n",
        "        :param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n",
        "        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
        "        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n",
        "        :param optimizer_class: Optimizer\n",
        "        :param optimizer_params: Optimizer parameters\n",
        "        :param weight_decay: Weight decay for model parameters\n",
        "        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n",
        "        :param output_path: Storage path for the model and evaluation files\n",
        "        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n",
        "        :param max_grad_norm: Used for gradient normalization.\n",
        "        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n",
        "        :param callback: Callback function that is invoked after each evaluation.\n",
        "                It must accept the following three parameters in this order:\n",
        "                `score`, `epoch`, `steps`\n",
        "        :param show_progress_bar: If True, output a tqdm progress bar\n",
        "        :param checkpoint_path: Folder to save checkpoints during training\n",
        "        :param checkpoint_save_steps: Will save a checkpoint after so many steps\n",
        "        :param checkpoint_save_total_limit: Total number of checkpoints to store\n",
        "        \"\"\"\n",
        "\n",
        "        #print(\"Wenyi added this!\")\n",
        "        #print(\"I am in the fit function of Wenyi's sentence transformer\")\n",
        "\n",
        "        ##Add info to model card\n",
        "        #info_loss_functions = \"\\n\".join([\"- {} with {} training examples\".format(str(loss), len(dataloader)) for dataloader, loss in train_objectives])\n",
        "        info_loss_functions =  []\n",
        "        for dataloader, loss in train_objectives:\n",
        "            info_loss_functions.extend(ModelCardTemplate.get_train_objective_info(dataloader, loss))\n",
        "        info_loss_functions = \"\\n\\n\".join([text for text in info_loss_functions])\n",
        "\n",
        "        info_fit_parameters = json.dumps({\"evaluator\": fullname(evaluator), \"epochs\": epochs, \"steps_per_epoch\": steps_per_epoch, \"scheduler\": scheduler, \"warmup_steps\": warmup_steps, \"optimizer_class\": str(optimizer_class),  \"optimizer_params\": optimizer_params, \"weight_decay\": weight_decay, \"evaluation_steps\": evaluation_steps, \"max_grad_norm\": max_grad_norm }, indent=4, sort_keys=True)\n",
        "        self._model_card_text = None\n",
        "        self._model_card_vars['{TRAINING_SECTION}'] = ModelCardTemplate.__TRAINING_SECTION__.replace(\"{LOSS_FUNCTIONS}\", info_loss_functions).replace(\"{FIT_PARAMETERS}\", info_fit_parameters)\n",
        "\n",
        "\n",
        "        if use_amp:\n",
        "            from torch.cuda.amp import autocast\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        self.to(self._target_device)\n",
        "\n",
        "        dataloaders = [dataloader for dataloader, _ in train_objectives]\n",
        "\n",
        "        # Use smart batching\n",
        "        for dataloader in dataloaders:\n",
        "            dataloader.collate_fn = self.smart_batching_collate\n",
        "\n",
        "        loss_models = [loss for _, loss in train_objectives]\n",
        "        for loss_model in loss_models:\n",
        "            loss_model.to(self._target_device)\n",
        "\n",
        "        self.best_score = -9999999\n",
        "\n",
        "        if steps_per_epoch is None or steps_per_epoch == 0:\n",
        "            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n",
        "\n",
        "        num_train_steps = int(steps_per_epoch * epochs)\n",
        "\n",
        "        # Prepare optimizers\n",
        "        optimizers = []\n",
        "        schedulers = []\n",
        "        for loss_model in loss_models:\n",
        "            param_optimizer = list(loss_model.named_parameters())\n",
        "\n",
        "            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
        "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "            ]\n",
        "\n",
        "            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n",
        "            scheduler_obj = self._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n",
        "\n",
        "            optimizers.append(optimizer)\n",
        "            schedulers.append(scheduler_obj)\n",
        "\n",
        "\n",
        "        global_step = 0\n",
        "        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n",
        "\n",
        "        num_train_objectives = len(train_objectives)\n",
        "\n",
        "        skip_scheduler = False\n",
        "        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n",
        "            training_steps = 0\n",
        "\n",
        "            ## Wenyi added this to track training loss <start> \n",
        "            ## Reset the total loss for this epoch.\n",
        "            total_train_loss = 0\n",
        "            ## <end>\n",
        "\n",
        "            for loss_model in loss_models:\n",
        "                loss_model.zero_grad()\n",
        "                loss_model.train()\n",
        "\n",
        "            for _ in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n",
        "                for train_idx in range(num_train_objectives):\n",
        "                    loss_model = loss_models[train_idx]\n",
        "                    optimizer = optimizers[train_idx]\n",
        "                    scheduler = schedulers[train_idx]\n",
        "                    data_iterator = data_iterators[train_idx]\n",
        "\n",
        "                    try:\n",
        "                        data = next(data_iterator)\n",
        "                    except StopIteration:\n",
        "                        data_iterator = iter(dataloaders[train_idx])\n",
        "                        data_iterators[train_idx] = data_iterator\n",
        "                        data = next(data_iterator)\n",
        "\n",
        "\n",
        "                    features, labels = data\n",
        "\n",
        "\n",
        "                    if use_amp:\n",
        "                        with autocast():\n",
        "                            loss_value = loss_model(features, labels)\n",
        "\n",
        "                        scale_before_step = scaler.get_scale()\n",
        "                        scaler.scale(loss_value).backward()\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        skip_scheduler = scaler.get_scale() != scale_before_step\n",
        "                    else:\n",
        "                        loss_value = loss_model(features, labels)\n",
        "                        loss_value.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n",
        "                        optimizer.step()\n",
        "                    \n",
        "                    # print(\"Adding the loss to the total loss\")\n",
        "                    # print(loss_value)\n",
        "                    total_train_loss = total_train_loss + loss_value\n",
        "                    # print(\"---\")\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    if not skip_scheduler:\n",
        "                        scheduler.step()\n",
        " \n",
        "\n",
        "                training_steps += 1\n",
        "                global_step += 1\n",
        "\n",
        "                if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n",
        "                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n",
        "\n",
        "                    for loss_model in loss_models:\n",
        "                        loss_model.zero_grad()\n",
        "                        loss_model.train()\n",
        "\n",
        "                if checkpoint_path is not None and checkpoint_save_steps is not None and checkpoint_save_steps > 0 and global_step % checkpoint_save_steps == 0:\n",
        "                    self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "\n",
        "            # Wenyi added... Calculate the average loss over all of the batches.\n",
        "            avg_train_loss = total_train_loss / len(dataloaders[0])\n",
        "            logger.info(\"Length Dataloaders:         \\t{:.2f}\".format(len(dataloaders[0])))\n",
        "            logger.info(\"Average training loss:      \\t{:.2f}\\n\".format(avg_train_loss))\n",
        "            \n",
        "\n",
        "\n",
        "            self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n",
        "\n",
        "        if evaluator is None and output_path is not None:   #No evaluator, but output path: save final model version\n",
        "            self.save(output_path)\n",
        "\n",
        "        if checkpoint_path is not None:\n",
        "            self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, evaluator: SentenceEvaluator, output_path: str = None):\n",
        "        \"\"\"\n",
        "        Evaluate the model\n",
        "\n",
        "        :param evaluator:\n",
        "            the evaluator\n",
        "        :param output_path:\n",
        "            the evaluator can write the results to this path\n",
        "        \"\"\"\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "        return evaluator(self, output_path)\n",
        "\n",
        "    def _eval_during_training(self, evaluator, output_path, save_best_model, epoch, steps, callback):\n",
        "        \"\"\"Runs evaluation during the training\"\"\"\n",
        "        eval_path = output_path\n",
        "        if output_path is not None:\n",
        "            os.makedirs(output_path, exist_ok=True)\n",
        "            eval_path = os.path.join(output_path, \"eval\")\n",
        "            os.makedirs(eval_path, exist_ok=True)\n",
        "\n",
        "        if evaluator is not None:\n",
        "            score = evaluator(self, output_path=eval_path, epoch=epoch, steps=steps)\n",
        "\n",
        "            if callback is not None:\n",
        "                callback(score, epoch, steps)\n",
        "            if score > self.best_score:\n",
        "                self.best_score = score\n",
        "                if save_best_model:\n",
        "                    self.save(output_path)\n",
        "\n",
        "    def _save_checkpoint(self, checkpoint_path, checkpoint_save_total_limit, step):\n",
        "        # Store new checkpoint\n",
        "        self.save(os.path.join(checkpoint_path, str(step)))\n",
        "\n",
        "        # Delete old checkpoints\n",
        "        if checkpoint_save_total_limit is not None and checkpoint_save_total_limit > 0:\n",
        "            old_checkpoints = []\n",
        "            for subdir in os.listdir(checkpoint_path):\n",
        "                if subdir.isdigit():\n",
        "                    old_checkpoints.append({'step': int(subdir), 'path': os.path.join(checkpoint_path, subdir)})\n",
        "\n",
        "            if len(old_checkpoints) > checkpoint_save_total_limit:\n",
        "                old_checkpoints = sorted(old_checkpoints, key=lambda x: x['step'])\n",
        "                shutil.rmtree(old_checkpoints[0]['path'])\n",
        "\n",
        "\n",
        "    def _load_auto_model(self, model_name_or_path):\n",
        "        \"\"\"\n",
        "        Creates a simple Transformer + Mean Pooling model and returns the modules\n",
        "        \"\"\"\n",
        "        logger.warning(\"No sentence-transformers model found with name {}. Creating a new one with MEAN pooling.\".format(model_name_or_path))\n",
        "        transformer_model = Transformer(model_name_or_path)\n",
        "        pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), 'mean')\n",
        "        return [transformer_model, pooling_model]\n",
        "\n",
        "    def _load_sbert_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Loads a full sentence-transformers model\n",
        "        \"\"\"\n",
        "        # Check if the config_sentence_transformers.json file exists (exists since v2 of the framework)\n",
        "        config_sentence_transformers_json_path = os.path.join(model_path, 'config_sentence_transformers.json')\n",
        "        if os.path.exists(config_sentence_transformers_json_path):\n",
        "            with open(config_sentence_transformers_json_path) as fIn:\n",
        "                self._model_config = json.load(fIn)\n",
        "\n",
        "            if '__version__' in self._model_config and 'sentence_transformers' in self._model_config['__version__'] and self._model_config['__version__']['sentence_transformers'] > __version__:\n",
        "                logger.warning(\"You try to use a model that was created with version {}, however, your version is {}. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\\n\\n\\n\".format(self._model_config['__version__']['sentence_transformers'], __version__))\n",
        "\n",
        "        # Check if a readme exists\n",
        "        model_card_path = os.path.join(model_path, 'README.md')\n",
        "        if os.path.exists(model_card_path):\n",
        "            try:\n",
        "                with open(model_card_path, encoding='utf8') as fIn:\n",
        "                    self._model_card_text = fIn.read()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Load the modules of sentence transformer\n",
        "        modules_json_path = os.path.join(model_path, 'modules.json')\n",
        "        with open(modules_json_path) as fIn:\n",
        "            modules_config = json.load(fIn)\n",
        "\n",
        "        modules = OrderedDict()\n",
        "        for module_config in modules_config:\n",
        "            module_class = import_from_string(module_config['type'])\n",
        "            module = module_class.load(os.path.join(model_path, module_config['path']))\n",
        "            modules[module_config['name']] = module\n",
        "\n",
        "        return modules\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_scheduler(optimizer, scheduler: str, warmup_steps: int, t_total: int):\n",
        "        \"\"\"\n",
        "        Returns the correct learning rate scheduler. Available scheduler: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n",
        "        \"\"\"\n",
        "        scheduler = scheduler.lower()\n",
        "        if scheduler == 'constantlr':\n",
        "            return transformers.get_constant_schedule(optimizer)\n",
        "        elif scheduler == 'warmupconstant':\n",
        "            return transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "        elif scheduler == 'warmuplinear':\n",
        "            return transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        elif scheduler == 'warmupcosine':\n",
        "            return transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        elif scheduler == 'warmupcosinewithhardrestarts':\n",
        "            return transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown scheduler {}\".format(scheduler))\n",
        "\n",
        "    @property\n",
        "    def device(self) -> device:\n",
        "        \"\"\"\n",
        "        Get torch.device from module, assuming that the whole module has one device.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return next(self.parameters()).device\n",
        "        except StopIteration:\n",
        "            # For nn.DataParallel compatibility in PyTorch 1.5\n",
        "\n",
        "            def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n",
        "                tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
        "                return tuples\n",
        "\n",
        "            gen = self._named_members(get_members_fn=find_tensor_attributes)\n",
        "            first_tuple = next(gen)\n",
        "            return first_tuple[1].device\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        \"\"\"\n",
        "        Property to get the tokenizer that is used by this model\n",
        "        \"\"\"\n",
        "        return self._first_module().tokenizer\n",
        "\n",
        "    @tokenizer.setter\n",
        "    def tokenizer(self, value):\n",
        "        \"\"\"\n",
        "        Property to set the tokenizer that should be used by this model\n",
        "        \"\"\"\n",
        "        self._first_module().tokenizer = value\n",
        "\n",
        "    @property\n",
        "    def max_seq_length(self):\n",
        "        \"\"\"\n",
        "        Property to get the maximal input sequence length for the model. Longer inputs will be truncated.\n",
        "        \"\"\"\n",
        "        return self._first_module().max_seq_length\n",
        "\n",
        "    @max_seq_length.setter\n",
        "    def max_seq_length(self, value):\n",
        "        \"\"\"\n",
        "        Property to set the maximal input sequence length for the model. Longer inputs will be truncated.\n",
        "        \"\"\"\n",
        "        self._first_module().max_seq_length = value"
      ],
      "metadata": {
        "id": "bMtXuhNNCGTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WUzH1gDykBiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ekFct7mVkBlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v7jgAYYgkBpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O-mUNgzjkBsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import My Metric From Github"
      ],
      "metadata": {
        "id": "s9hybrtWe2He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/wenyi-tay/moc.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNIhzPF7e737",
        "outputId": "e44e4fb2-75f3-45d5-9d00-92f9b526374b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'moc'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 20 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd moc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgz4Q7fg7hF",
        "outputId": "8a4df7c4-16c9-4d41-d8c2-19cbc4350b0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/moc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mnl1nIMhJxh",
        "outputId": "c54c2465-1521-4178-dd1c-27d58f4c7591"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  \u001b[01;34m.git\u001b[0m/  .gitignore  LICENSE  moc.py  README.md  SOS_LT_Sentences.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import moc\n",
        "from moc import global_coherence_sos, inspect_summary"
      ],
      "metadata": {
        "id": "fpwy5JTThPrx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"hello! This is crappy. I need to fix this.\""
      ],
      "metadata": {
        "id": "zCW8CtNghhWN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c = global_coherence_sos(summary, inspect = True, k = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "RJTx_o0shme0",
        "outputId": "728e2728-e7eb-49ff-c863-3f685bd4e75f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences of summary:\n",
            "S0: hello!\n",
            "S1: This is crappy.\n",
            "S2: I need to fix this.\n",
            "\n",
            "\n",
            "Sentence least similar to other sentences in summary:\n",
            "S0: hello!\n",
            "\n",
            "\n",
            "1 sentence pair(s) with lowest similarity scores:\n",
            "Similarity score: -0.28303885\n",
            "S0: hello!\n",
            "S1: This is crappy.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c = global_coherence_sos(summary, inspect = False, k = 1)"
      ],
      "metadata": {
        "id": "09iOrMjchtbq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "voJ3vMYMiLo3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}